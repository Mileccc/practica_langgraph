{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "import os\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from typing import TypedDict, Annotated, Literal\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "os.environ['TAVILY_API_KEY'] = os.getenv('TAVILY_API_KEY')\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = os.getenv('LANGCHAIN_TRACING_V2')\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = os.getenv('LANGCHAIN_ENDPOINT')\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv('LANGCHAIN_API_KEY')\n",
    "os.environ['LANGCHAIN_PROJECT'] = os.getenv('LANGCHAIN_PROJECT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [TavilySearchResults(max_results=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0, streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_node = ToolNode(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_messages(left: list, right: list):\n",
    "    \"\"\"Añadir sin sobrescribir.\"\"\"\n",
    "    return left + right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir la función que determina si continuar o no\n",
    "def should_continue(state: AgentState) -> Literal[\"tools\", \"__end__\"]:\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1]\n",
    "    # Si el LLM hace una llamada a una herramienta, entonces enrutamos al nodo \"tools\"\n",
    "    if last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    # De lo contrario, nos detenemos (respondemos al usuario)\n",
    "    return \"__end__\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir la función que llama al modelo\n",
    "def call_model(state: AgentState):\n",
    "    messages = state['messages']\n",
    "    \n",
    "    response = model.invoke(messages)\n",
    " \n",
    "    # Devolvemos una lista, porque esto se agregará a la lista existente\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definir el Grafo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir un nuevo grafo\n",
    "workflow = StateGraph(AgentState)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir los dos nodos entre los que ciclará\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"tools\", tool_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establecer el punto de entrada como `agent`\n",
    "workflow.set_entry_point(\"agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregar un borde condicional\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregar un borde normal de `tools` a `agent`\n",
    "workflow.add_edge('tools', 'agent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalmente, ¡compilamos!\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "mensaje = HumanMessage(content=\"¿Cuál es el clima en San Francisco? Contesta en español\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='¿Cuál es el clima en San Francisco? Contesta en español')]}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = {\"messages\": [mensaje]}\n",
    "inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salida del Nodo 'agent':\n",
      "---\n",
      "{'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_QTK7gdFGDJjtLzdRjkNuk832', 'function': {'arguments': '{\"query\":\"clima en San Francisco\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls'}, id='run-44bd3ea7-1b22-4762-afc3-462af1d7585c-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'clima en San Francisco'}, 'id': 'call_QTK7gdFGDJjtLzdRjkNuk832'}])]}\n",
      "\n",
      "---\n",
      "\n",
      "Salida del Nodo 'tools':\n",
      "---\n",
      "{'messages': [ToolMessage(content='[{\"url\": \"https://weather.com/es-US/tiempo/10dias/l/San+Francisco+CA+USCA0987:1:US/\", \"content\": \"Prep\\\\u00e1rate con el pron\\\\u00f3stico para los pr\\\\u00f3ximos 10 d\\\\u00edas m\\\\u00e1s preciso para San Francisco, CA. Consulta la temperatura m\\\\u00e1xima y m\\\\u00ednima y la probabilidad de lluvia en The Weather Channel y ...\"}]', name='tavily_search_results_json', tool_call_id='call_QTK7gdFGDJjtLzdRjkNuk832')]}\n",
      "\n",
      "---\n",
      "\n",
      "Salida del Nodo 'agent':\n",
      "---\n",
      "{'messages': [AIMessage(content='Puedes encontrar el pronóstico del clima para San Francisco en los próximos 10 días en The Weather Channel. Para más detalles, puedes visitar el siguiente enlace: [Pronóstico del clima en San Francisco](https://weather.com/es-US/tiempo/10dias/l/San+Francisco+CA+USCA0987:1:US/)', response_metadata={'finish_reason': 'stop'}, id='run-5450e609-e949-49dc-b91a-20532e48f4a6-0')]}\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for output in app.stream(inputs, stream_mode=\"updates\"):\n",
    "    # stream() yields dictionaries with output keyed by node name\n",
    "    for key, value in output.items():\n",
    "        print(f\"Salida del Nodo '{key}':\")\n",
    "        print(\"---\")\n",
    "        print(value)\n",
    "    print(\"\\n---\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v2email",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
